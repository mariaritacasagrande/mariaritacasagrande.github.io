---
layout: post
title: Algoritimos, Inteligencia artificial e mulheres - Quando a História se repete!
featured: true
author: ahmad
tags: [frontpage, jekyll, blog]
image: '/images/posts/algoritmo-mujeres.jpg'
---

Temos que ensinar que a tecnologia não tem gênero. Enquanto repetimos essa frase como um mantra, os algoritmos já causaram muitos danos. No artigo anterior Algoritmos e inteligência artificial: Por onde começar ?, Referia-se ao uso de dados sujos pela polícia americana, que reforçava práticas discriminatórias. Esse é um grande problema que nos leva a pensar que por trás de cada algoritmo há dados, dados introduzidos por indivíduos, muitas vezes com seus próprios interesses. Exemplos como esse aparecem nos noticiários quase todos os dias. Não se surpreenda, portanto, com os resultados quando começarmos a investigar o papel das mulheres diante das novas tecnologias e particularmente diante dos algoritmos.

>Acreditar que ganhamos a guerra da igualdade porque o abuso sexual, o abuso e a violência contra as mulheres são cada vez mais denunciados parece esquecer que os algoritmos e a inteligência artificial que estão mudando nossas vidas também podem se tornar uma arma de discriminação em massa.

	<h2>Discriminação, preconceito, injustiça, o impacto do viés nos algoritmos</h2>
  
  Uma busca com as palavras Algoritmo e mulher em inglês no Google é suficiente para entender o problema principal (convido você a fazer a mesma busca mudando mulher por homem).

O primeiro resultado é um artigo intitulado "Incluindo mulheres em IA e algoritmos" da Fundação World Wide Web de Tim Berners-Lee, o criador da Internet e do hiperlink. A necessidade de publicar esse conteúdo já nos faz suspeitar que estamos diante de um problema. O artigo refere-se a um relatório que alerta sobre a dominação quase exclusiva dos homens no campo da inteligência artificial. Para reduzir essa lacuna, os autores do relatório propõem três recomendações: as mulheres, em primeiro lugar, devem ter um papel ativo na modelagem da nova geração de tecnologia; então os estados têm que implementar linhas de conduta para proteger as mulheres de algoritmos discriminatórios; e, finalmente, a pesquisa sobre o impacto dos algoritmos nas mulheres precisa ser fortalecida.

![Pesquisa no Google](/images/posts/google-search.jpg "Psquisa Google")

Os resultados a seguir dessa pesquisa confirmam a necessidade urgente de agir antes que a mesma história seja repetida. No artigo da revista Wired, Máquinas ensinadas por fotos aprendem uma visão sexista das mulheres, a existência da diferença de gênero é confirmada. Os resultados do estudo da Universidade da Virgínia, detalhados no artigo do El País Se está na cozinha, é uma mulher: como algoritmos reforçam preconceitos, mostram que após o treinamento um algoritmo com dois bancos de imagens que contêm apenas em um terço do total que um homem cozinha, o programa de computador deduz em 83% dos casos que uma pessoa cozinha é uma mulher. Outro exemplo de estudo, no mesmo artigo da revista Wired, é feito em 2016 por pesquisadores da Universidade de Boston e da Microsoft. Eles mostraram, coletando textos do Google News, o viés de gênero: quando a máquina treinada é solicitada a completar a seguinte frase: "O equivalente a 'homem é programar o que a mulher é para ...'" ("O homem é programador de computador como mulher é para X, "), responde" a dona de casa ".

Esta situação, segundo o The Guardian, não é nova. No artigo Por que o problema de gênero da tecnologia não é novidade, o jornal britânico revela as práticas da Amazônia gigante. Desde 2014, a tecnologia de automação da oferta de emprego desenvolvida pela empresa de Jeff Bezos tem sido principalmente treinada com currículo masculino. De fato, o sistema aprendeu que candidatos do sexo masculino eram preferíveis. A explicação é simples: os modelos de computador da Amazon foram treinados observando padrões nos currículos apresentados à empresa durante um período de 10 anos. A maioria veio de homens.

	<h2>Quando a mudança é esperada</h2>
  
  A identificação e modificação de um algoritmo tendencioso parece ser complexa. Isto é explicado, da revista de tecnologia MIT Technology Review, Karen Hao, em Como o viés algorítmico é produzido e por que é tão difícil pará-lo. O autor qualifica o papel dos dados. O viés, segundo Hao, pode existir muito antes que os dados sejam coletados e em outros processos. Existem três etapas principais em que o viés algorítmico ocorre. A definição do objetivo, a primeira etapa da elaboração de um algoritmo, corresponde à transformação de um conceito em uma fórmula matemática. Obviamente, as características são determinadas de acordo com os interesses da empresa que o algoritmo precisa. A próxima etapa é introduzir dados que possam revelar preconceitos e não refletir a realidade, como vimos antes com as imagens de homens cozinhando. Na última etapa, o computador tem que escolher um atributo, isto é, uma característica particular que ajudará a dar mais precisão à previsão do algoritmo. A escolha de um atributo ou o descarte de outro pode influenciar totalmente o viés.

É possível combater o preconceito? Muitos elementos devem ser levados em conta, assim como o contexto social, por exemplo. Convido você a descobrir como Joy Buolamwini, pesquisadora do MIT Media Lab, luta contra o viés dos algoritmos. Alerta de Spoiler: pode ser combatido com a ajuda de pessoas.

<div style="max-width:854px"><div style="position:relative;height:0;padding-bottom:56.25%"><iframe src="https://embed.ted.com/talks/lang/en/joy_buolamwini_how_i_m_fighting_bias_in_algorithms" width="854" height="480" style="position:absolute;left:0;top:0;width:100%;height:100%" frameborder="0" scrolling="no" allowfullscreen></iframe></div></div>
